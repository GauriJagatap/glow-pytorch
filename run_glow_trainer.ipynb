{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import vision\n",
    "from docopt import docopt\n",
    "from torchvision import transforms\n",
    "from glow.builder_new import build\n",
    "from glow.trainer import Trainer\n",
    "from glow.config import JsonConfig\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glow.models_new import Glow\n",
    "from glow import learning_rate_schedule\n",
    "from glow import thops\n",
    "from glow.utils import get_proper_device\n",
    "from glow.utils import save\n",
    "import datetime\n",
    "from platform import python_version\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.6\n"
     ]
    }
   ],
   "source": [
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data and transform - CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to parse all image attrs\n",
      "Find 202599 images, with 40 attrs\n",
      "number of batches: 22511\n"
     ]
    }
   ],
   "source": [
    "hparams = JsonConfig('./hparams/celeba.json')\n",
    "dataset_name = 'celeba'\n",
    "if dataset_name == 'celeba':\n",
    "    dataset_root = 'dataset/CelebA'\n",
    "    dataset = vision.Datasets[dataset_name]\n",
    "    # set transform of dataset\n",
    "    transform = transforms.Compose([\n",
    "            transforms.CenterCrop(hparams.Data.center_crop),\n",
    "            transforms.Resize(hparams.Data.resize),\n",
    "            transforms.ToTensor()])\n",
    "    dataset = dataset(dataset_root, transform=transform)\n",
    "    data_loader = DataLoader(dataset,batch_size=hparams.Train.batch_size,shuffle=False,drop_last=True)\n",
    "    num_batches = len(data_loader)\n",
    "    print('number of batches:', num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data - sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pde'\n",
    "if dataset_name == 'pde':\n",
    "    class NumpyDataset(Dataset):\n",
    "        def __init__(self, path):\n",
    "            super().__init__()\n",
    "            self.data = np.load(path)\n",
    "            self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.transform(np.float32(self.data[idx]))\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.data.shape[0]\n",
    "\n",
    "    def get_dataloader(path, batchsize):\n",
    "        ds = NumpyDataset(path)\n",
    "        dl = DataLoader(ds, batch_size=batchsize, drop_last=True, shuffle=True)\n",
    "        return dl\n",
    "    data_loader = get_dataloader('pdedata/data_lowranknoise.npy',8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 1250\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(data_loader)\n",
    "print('number of batches:', num_batches)\n",
    "hparams = JsonConfig('./hparams/pde.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize Glow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Builder]: Found 1 gpu\n",
      "[Builder]: cuda:1 is not found, ignore.\n",
      "[Builder]: cuda:2 is not found, ignore.\n",
      "[Builder]: cuda:3 is not found, ignore.\n"
     ]
    }
   ],
   "source": [
    "Glownet = Glow(hparams)\n",
    "Glownet.device = hparams.Device.glow\n",
    "devices = get_proper_device(hparams.Device.glow)\n",
    "if len(devices) > 0:\n",
    "    device = Glownet.device[0]\n",
    "    Glownet = Glownet.to(device)\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  Dir {\n",
      "    log_root: results/pde\n",
      "  }\n",
      "  Glow {\n",
      "    image_shape: [64, 64, 1]\n",
      "    hidden_channels: 512\n",
      "    K: 32\n",
      "    L: 3\n",
      "    actnorm_scale: 1.0\n",
      "    flow_permutation: invconv\n",
      "    flow_coupling: affine\n",
      "    LU_decomposed: False\n",
      "    learn_top: False\n",
      "    y_condition: False\n",
      "    y_classes: 40\n",
      "  }\n",
      "  Criterion {\n",
      "    y_condition: multi-classes\n",
      "  }\n",
      "  Data {\n",
      "    center_crop: 160\n",
      "    resize: 64\n",
      "  }\n",
      "  Optim {\n",
      "    name: adam\n",
      "      args {\n",
      "      lr: 0.0001\n",
      "      betas: [0.9, 0.9999]\n",
      "      eps: 1e-08\n",
      "    }\n",
      "      Schedule {\n",
      "      name: noam_learning_rate_decay\n",
      "          args {\n",
      "        warmup_steps: 1000\n",
      "        minimum: 0.0001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  Device {\n",
      "    glow: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "    data: cuda:0\n",
      "  }\n",
      "  Train {\n",
      "    batch_size: 8\n",
      "    num_batches: 10000\n",
      "    max_grad_clip: 5\n",
      "    max_grad_norm: 100\n",
      "    max_checkpoints: 20\n",
      "    checkpoints_gap: 5000\n",
      "    num_plot_samples: 1\n",
      "    scalar_log_gap: 20\n",
      "    plot_gap: 20\n",
      "    inference_gap: 20\n",
      "    warm_start: \n",
      "    weight_y: 0.5\n",
      "  }\n",
      "  Infer {\n",
      "    pre_trained: ./results/pde/trained.pkg\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "date = str(datetime.datetime.now())\n",
    "date = date[:date.rfind(\":\")].replace(\"-\", \"\")\\\n",
    "                                     .replace(\":\", \"\")\\\n",
    "                                     .replace(\" \", \"_\")\n",
    "log_dir = os.path.join(hparams.Dir.log_root, \"log_\" + date)\n",
    "checkpoints_dir = os.path.join(log_dir, \"checkpoints\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "# write hparams\n",
    "hparams.dump(log_dir)\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_name = \"default\"\n",
    "schedule_args = {}\n",
    "if \"Schedule\" in hparams.Optim:\n",
    "    schedule_name = hparams.Optim.Schedule.name\n",
    "    schedule_args = hparams.Optim.Schedule.args.to_dict()\n",
    "if not (\"init_lr\" in schedule_args):\n",
    "        schedule_args[\"init_lr\"] = hparams.Optim.args.lr\n",
    "assert schedule_args[\"init_lr\"] == hparams.Optim.args.lr,\\\n",
    "                \"Optim lr {} != Schedule init_lr {}\".format(hparams.Optim.args.lr, schedule_args[\"init_lr\"])\n",
    "lrschedule = {\n",
    "                \"func\": getattr(learning_rate_schedule, schedule_name),\n",
    "                \"args\": schedule_args\n",
    "              }\n",
    "opt_params = hparams.Optim.args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_name = hparams.Optim.name\n",
    "if optim_name == 'adam':\n",
    "    optimizer = torch.optim.Adam(Glownet.parameters(), opt_params['lr'], opt_params['betas'], opt_params['eps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train Glow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.: 0\n",
      "epoch no. 0 , batch no. 0  of  1250\n",
      "generative loss: 4.115608\n",
      "norm of gradients: 156.08403423899406\n",
      "epoch no. 0 , batch no. 20  of  1250\n",
      "generative loss: 4.0624237\n",
      "norm of gradients: 72.67700821575286\n",
      "epoch no. 0 , batch no. 40  of  1250\n",
      "generative loss: 4.0241942\n",
      "norm of gradients: 57.897531735818205\n",
      "epoch no. 0 , batch no. 60  of  1250\n",
      "generative loss: 3.9914937\n",
      "norm of gradients: 54.73581612716947\n",
      "epoch no. 0 , batch no. 80  of  1250\n",
      "generative loss: 3.9562435\n",
      "norm of gradients: 56.825706512769266\n",
      "epoch no. 0 , batch no. 100  of  1250\n",
      "generative loss: 3.9247472\n",
      "norm of gradients: 50.240040211846804\n",
      "epoch no. 0 , batch no. 120  of  1250\n",
      "generative loss: 3.8969753\n",
      "norm of gradients: 44.43958457123058\n",
      "epoch no. 0 , batch no. 140  of  1250\n",
      "generative loss: 3.8680391\n",
      "norm of gradients: 41.67861898419261\n",
      "epoch no. 0 , batch no. 160  of  1250\n",
      "generative loss: 3.843376\n",
      "norm of gradients: 40.864850461710084\n",
      "epoch no. 0 , batch no. 180  of  1250\n",
      "generative loss: 3.8248544\n",
      "norm of gradients: 41.05496339041266\n",
      "epoch no. 0 , batch no. 200  of  1250\n",
      "generative loss: 3.7923021\n",
      "norm of gradients: 42.48535596647972\n",
      "epoch no. 0 , batch no. 220  of  1250\n",
      "generative loss: 3.762783\n",
      "norm of gradients: 36.62081852687491\n",
      "epoch no. 0 , batch no. 240  of  1250\n",
      "generative loss: 3.7423482\n",
      "norm of gradients: 36.2430822610156\n",
      "epoch no. 0 , batch no. 260  of  1250\n",
      "generative loss: 3.7178895\n",
      "norm of gradients: 41.48401106550001\n",
      "epoch no. 0 , batch no. 280  of  1250\n",
      "generative loss: 3.6910157\n",
      "norm of gradients: 40.88867113157039\n",
      "epoch no. 0 , batch no. 300  of  1250\n",
      "generative loss: 3.660932\n",
      "norm of gradients: 35.27199991982638\n",
      "epoch no. 0 , batch no. 320  of  1250\n",
      "generative loss: 3.6369603\n",
      "norm of gradients: 35.49448727319073\n",
      "epoch no. 0 , batch no. 340  of  1250\n",
      "generative loss: 3.6087894\n",
      "norm of gradients: 43.57254443396309\n",
      "epoch no. 0 , batch no. 360  of  1250\n",
      "generative loss: 3.5767274\n",
      "norm of gradients: 45.01025067509449\n",
      "epoch no. 0 , batch no. 380  of  1250\n",
      "generative loss: 3.5475693\n",
      "norm of gradients: 38.553014333387246\n",
      "epoch no. 0 , batch no. 400  of  1250\n",
      "generative loss: 3.5066938\n",
      "norm of gradients: 39.53587299842591\n",
      "epoch no. 0 , batch no. 420  of  1250\n",
      "generative loss: 3.468152\n",
      "norm of gradients: 48.08155650519092\n",
      "epoch no. 0 , batch no. 440  of  1250\n",
      "generative loss: 3.415676\n",
      "norm of gradients: 66.8530678685939\n",
      "epoch no. 0 , batch no. 441  of  1250\r"
     ]
    }
   ],
   "source": [
    "# initialize global_step : cumulative no. of optimizer steps = no. epochs * no. batches\n",
    "global_step = 0 \n",
    "generative_loss_perNepoch = []\n",
    "classification_loss_perNepoch = []\n",
    "check_images = False\n",
    "for epoch in range(10):\n",
    "    print(\"epoch no.:\", epoch)\n",
    "    for i_batch, batch in enumerate(data_loader):\n",
    "        print('epoch no.',epoch,', batch no.',i_batch,' of ', num_batches, end='\\r')\n",
    "        \n",
    "        # update learning rate\n",
    "        lr = lrschedule[\"func\"](global_step=0,**lrschedule[\"args\"])\n",
    "        #print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        # clear gradients for current mini-batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # send data to device and extract\n",
    "        if dataset_name=='celeba':\n",
    "            for k in batch:\n",
    "                print(k.shape)\n",
    "                batch[k] = batch[k].to(device)\n",
    "        elif dataset_name=='pde':\n",
    "            x = batch.to(device)\n",
    "            \n",
    "        # extract images x\n",
    "        if dataset_name =='celeba':\n",
    "            x = batch[\"x\"]\n",
    "            \n",
    "        # extract labels y, y_onehot\n",
    "        y = None\n",
    "        y_onehot = None\n",
    "        if hparams.Glow.y_condition:\n",
    "            if hparams.Criterion.y_condition == \"multi-classes\":\n",
    "                assert \"y_onehot\" in batch, \"multi-classes ask for `y_onehot` (torch.FloatTensor onehot)\"\n",
    "                y_onehot = batch[\"y_onehot\"]\n",
    "            elif hparams.Criterion.y_condition == \"single-class\":\n",
    "                assert \"y\" in batch, \"single-class ask for `y` (torch.LongTensor indexes)\"\n",
    "                y = batch[\"y\"]\n",
    "                y_onehot = thops.onehot(y, num_classes=hparams.Glow.y_classes)\n",
    "\n",
    "        # initialize ActNorm (first iteration only)\n",
    "        if global_step == 0:\n",
    "            Glownet(x[:hparams.Train.batch_size // len(devices), ...],y_onehot[:hparams.Train.batch_size // len(devices), ...] if y_onehot is not None else None)\n",
    "        \n",
    "        # parallel \n",
    "        if len(devices) > 1 and not hasattr(Glownet, \"module\"):\n",
    "            print(\"[Parallel] move to {}\".format(self.devices))\n",
    "            self.graph = torch.nn.parallel.DataParallel(self.graph, self.devices, self.devices[0])\n",
    "            \n",
    "        # forward phase\n",
    "        z, nll, y_logits = Glownet(x=x, y_onehot=y_onehot)\n",
    "        \n",
    "        # construct genetative loss\n",
    "        loss_generative = Glownet.loss_generative(nll)\n",
    "        \n",
    "        # construct classification loss\n",
    "        loss_classes = 0\n",
    "        if hparams.Glow.y_condition:\n",
    "            loss_classes = (Glownet.loss_multi_classes(y_logits, y_onehot)\n",
    "                            if self.y_criterion == \"multi-classes\" else\n",
    "                                    Glownet.loss_class(y_logits, y))\n",
    "        \n",
    "        #construct overall loss function\n",
    "        if global_step % hparams.Train.scalar_log_gap == 0:\n",
    "            generative_loss_perNepoch.append(loss_generative)\n",
    "            print(\"\\ngenerative loss:\", loss_generative.detach().cpu().numpy())\n",
    "            if hparams.Glow.y_condition:\n",
    "                generative_loss_perNepoch.append(loss_generative)\n",
    "                print(\"classification loss:\", loss_classes)\n",
    "        loss = loss_generative + loss_classes * hparams.Train.weight_y\n",
    "\n",
    "        # backpropagate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients\n",
    "        if hparams.Train.max_grad_clip is not None and hparams.Train.max_grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_value_(Glownet.parameters(), hparams.Train.max_grad_clip)\n",
    "        if hparams.Train.max_grad_norm is not None and hparams.Train.max_grad_norm > 0:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(Glownet.parameters(), hparams.Train.max_grad_norm)\n",
    "            if global_step % hparams.Train.scalar_log_gap == 0:\n",
    "                print(\"norm of gradients:\", grad_norm)\n",
    "        \n",
    "        # gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # checkpoints\n",
    "        if global_step % hparams.Train.checkpoints_gap == 0 and global_step > 0:\n",
    "            save(global_step=global_step,\n",
    "                         graph=Glownet,\n",
    "                         optim=optimizer,\n",
    "                         pkg_dir=checkpoints_dir,\n",
    "                         is_best=True,\n",
    "                         max_checkpoints=hparams.Train.max_checkpoints)\n",
    "        \n",
    "        # check generated images and plot\n",
    "        if check_images:\n",
    "            if global_step % hparams.Train.plot_gap == 0:\n",
    "                img = Glownet(z=z, y_onehot=y_onehot, reverse=True)\n",
    "                # img = torch.clamp(img, min=0, max=1.0)\n",
    "                if hparams.Glow.y_condition:\n",
    "                    if hparams.Criterion.y_condition == \"multi-classes\":\n",
    "                        y_pred = torch.sigmoid(y_logits)\n",
    "                    elif hparams.Criterion.y_condition == \"single-class\":\n",
    "                        y_pred = thops.onehot(torch.argmax(F.softmax(y_logits, dim=1), dim=1, keepdim=True),\n",
    "                                                      self.y_classes)\n",
    "                    y_true = y_onehot\n",
    "\n",
    "                for bi in range(min([len(img), 4])):\n",
    "                    self.writer.add_image(\"0_reverse/{}\".format(bi), torch.cat((img[bi], batch[\"x\"][bi]), dim=1), self.global_step)\n",
    "                    if hparams.Glow.y_condition:\n",
    "                        self.writer.add_image(\"1_prob/{}\".format(bi), plot_prob([y_pred[bi], y_true[bi]], [\"pred\", \"true\"]), self.global_step)\n",
    "\n",
    "\n",
    "            # inference\n",
    "            if hparams.Train.inference_gap is not None:\n",
    "                if global_step % hparams.Train.inference_gap == 0:\n",
    "                    img = Glownet(z=None, y_onehot=y_onehot, eps_std=0.5, reverse=True)\n",
    "                    for bi in range(min([len(img), 4])):\n",
    "                        self.writer.add_image(\"2_sample/{}\".format(bi), img[bi], self.global_step)\n",
    "\n",
    "        # global step\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_images:\n",
    "    self.writer.export_scalars_to_json(os.path.join(self.log_dir, \"all_scalars.json\"))\n",
    "    self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.Train.checkpoints_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
