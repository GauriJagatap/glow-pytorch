{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import vision\n",
    "from docopt import docopt\n",
    "from torchvision import transforms\n",
    "from glow.builder_new import build\n",
    "from glow.trainer import Trainer\n",
    "from glow.config import JsonConfig\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glow.models_new import Glow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = JsonConfig('./hparams/celeba.json')\n",
    "dataset_name = 'celeba'\n",
    "dataset_root = 'dataset/CelebA'\n",
    "dataset = vision.Datasets[dataset_name]\n",
    "# set transform of dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.CenterCrop(hparams.Data.center_crop),\n",
    "        transforms.Resize(hparams.Data.resize),\n",
    "        transforms.ToTensor()])\n",
    "# build graph and dataset\n",
    "built = Glow(hparams)\n",
    "#built = build(hparams, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_name = \"default\"\n",
    "schedule_args = {}\n",
    "if \"Schedule\" in hparams.Optim:\n",
    "    schedule_name = hparams.Optim.Schedule.name\n",
    "    schedule_args = hparams.Optim.Schedule.args.to_dict()\n",
    "if not (\"init_lr\" in schedule_args):\n",
    "        schedule_args[\"init_lr\"] = hparams.Optim.args.lr\n",
    "assert schedule_args[\"init_lr\"] == hparams.Optim.args.lr,\\\n",
    "                \"Optim lr {} != Schedule init_lr {}\".format(hparams.Optim.args.lr, schedule_args[\"init_lr\"])\n",
    "lrschedule = {\n",
    "                \"func\": getattr(learning_rate_schedule, schedule_name),\n",
    "                \"args\": schedule_args\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_name = hparams.Optim.name\n",
    "if optim_name == 'Adam':\n",
    "    optimizer = torch.optim.Adam(graph.parameters(), hparams.Optim.args.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin to train\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch\", epoch)\n",
    "    for i_batch, batch in enumerate(DataLoader):\n",
    "        # update learning rate\n",
    "        lr = lrschedule[\"func\"](global_step=0,**lrschedule[\"args\"])\n",
    "        for param_group in self.optim.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        optimizer.zero_grad()\n",
    "        if self.global_step % self.scalar_log_gaps == 0:\n",
    "            self.writer.add_scalar(\"lr/lr\", lr, self.global_step)\n",
    "        # get batch data\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(self.data_device)\n",
    "        x = batch[\"x\"]\n",
    "        y = None\n",
    "        y_onehot = None\n",
    "        if self.y_condition:\n",
    "            if self.y_criterion == \"multi-classes\":\n",
    "                assert \"y_onehot\" in batch, \"multi-classes ask for `y_onehot` (torch.FloatTensor onehot)\"\n",
    "                y_onehot = batch[\"y_onehot\"]\n",
    "            elif self.y_criterion == \"single-class\":\n",
    "                assert \"y\" in batch, \"single-class ask for `y` (torch.LongTensor indexes)\"\n",
    "                y = batch[\"y\"]\n",
    "                y_onehot = thops.onehot(y, num_classes=self.y_classes)\n",
    "\n",
    "        # at first time, initialize ActNorm\n",
    "        if self.global_step == 0:\n",
    "            self.graph(x[:self.batch_size // len(self.devices), ...],\n",
    "                               y_onehot[:self.batch_size // len(self.devices), ...] if y_onehot is not None else None)\n",
    "        # parallel\n",
    "        if len(self.devices) > 1 and not hasattr(self.graph, \"module\"):\n",
    "            print(\"[Parallel] move to {}\".format(self.devices))\n",
    "            self.graph = torch.nn.parallel.DataParallel(self.graph, self.devices, self.devices[0])\n",
    "        # forward phase\n",
    "        z, nll, y_logits = self.graph(x=x, y_onehot=y_onehot)\n",
    "\n",
    "        # loss\n",
    "        loss_generative = Glow.loss_generative(nll)\n",
    "        loss_classes = 0\n",
    "        if self.y_condition:\n",
    "            loss_classes = (Glow.loss_multi_classes(y_logits, y_onehot)\n",
    "                            if self.y_criterion == \"multi-classes\" else\n",
    "                                    Glow.loss_class(y_logits, y))\n",
    "        if self.global_step % self.scalar_log_gaps == 0:\n",
    "            self.writer.add_scalar(\"loss/loss_generative\", loss_generative, self.global_step)\n",
    "            if self.y_condition:\n",
    "                self.writer.add_scalar(\"loss/loss_classes\", loss_classes, self.global_step)\n",
    "        loss = loss_generative + loss_classes * self.weight_y\n",
    "\n",
    "        # backward\n",
    "        self.graph.zero_grad()\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        # operate grad\n",
    "        if self.max_grad_clip is not None and self.max_grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_value_(self.graph.parameters(), self.max_grad_clip)\n",
    "        if self.max_grad_norm is not None and self.max_grad_norm > 0:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.graph.parameters(), self.max_grad_norm)\n",
    "            if self.global_step % self.scalar_log_gaps == 0:\n",
    "                self.writer.add_scalar(\"grad_norm/grad_norm\", grad_norm, self.global_step)\n",
    "        # step\n",
    "        self.optim.step()\n",
    "\n",
    "        # checkpoints\n",
    "        if self.global_step % self.checkpoints_gap == 0 and self.global_step > 0:\n",
    "            save(global_step=self.global_step,\n",
    "                         graph=self.graph,\n",
    "                         optim=self.optim,\n",
    "                         pkg_dir=self.checkpoints_dir,\n",
    "                         is_best=True,\n",
    "                         max_checkpoints=self.max_checkpoints)\n",
    "        if self.global_step % self.plot_gaps == 0:\n",
    "            img = self.graph(z=z, y_onehot=y_onehot, reverse=True)\n",
    "            # img = torch.clamp(img, min=0, max=1.0)\n",
    "            if self.y_condition:\n",
    "                if self.y_criterion == \"multi-classes\":\n",
    "                    y_pred = torch.sigmoid(y_logits)\n",
    "                elif self.y_criterion == \"single-class\":\n",
    "                    y_pred = thops.onehot(torch.argmax(F.softmax(y_logits, dim=1), dim=1, keepdim=True),\n",
    "                                                  self.y_classes)\n",
    "                y_true = y_onehot\n",
    "            for bi in range(min([len(img), 4])):\n",
    "                self.writer.add_image(\"0_reverse/{}\".format(bi), torch.cat((img[bi], batch[\"x\"][bi]), dim=1), self.global_step)\n",
    "                if self.y_condition:\n",
    "                    self.writer.add_image(\"1_prob/{}\".format(bi), plot_prob([y_pred[bi], y_true[bi]], [\"pred\", \"true\"]), self.global_step)\n",
    "\n",
    "        # inference\n",
    "        if hasattr(self, \"inference_gap\"):\n",
    "            if self.global_step % self.inference_gap == 0:\n",
    "                img = self.graph(z=None, y_onehot=y_onehot, eps_std=0.5, reverse=True)\n",
    "                # img = torch.clamp(img, min=0, max=1.0)\n",
    "                for bi in range(min([len(img), 4])):\n",
    "                    self.writer.add_image(\"2_sample/{}\".format(bi), img[bi], self.global_step)\n",
    "\n",
    "        # global step\n",
    "        self.global_step += 1\n",
    "\n",
    "self.writer.export_scalars_to_json(os.path.join(self.log_dir, \"all_scalars.json\"))\n",
    "self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset(dataset_root, transform=transform)\n",
    "# begin to train\n",
    "trainer = Trainer(**built, dataset=dataset, hparams=hparams)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
